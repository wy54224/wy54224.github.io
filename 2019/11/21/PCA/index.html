<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.5.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":true},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: true,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="Principal Component Analysis(PCA) 问题分析 主成分分析——寻找一个超平面，尽量将所有样本正确表达。">
<meta name="keywords" content="PCA,outlier detection">
<meta property="og:type" content="article">
<meta property="og:title" content="从PCA到概率图模型做Multi-view Outlier Detection（1）">
<meta property="og:url" content="https:&#x2F;&#x2F;wy54224.github.io&#x2F;2019&#x2F;11&#x2F;21&#x2F;PCA&#x2F;index.html">
<meta property="og:site_name" content="wavy的流动小屋">
<meta property="og:description" content="Principal Component Analysis(PCA) 问题分析 主成分分析——寻找一个超平面，尽量将所有样本正确表达。">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2019-11-23T04:32:55.557Z">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://wy54224.github.io/2019/11/21/PCA/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>从PCA到概率图模型做Multi-view Outlier Detection（1） | wavy的流动小屋</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">wavy的流动小屋</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">学习永不止境</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://wy54224.github.io/2019/11/21/PCA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="wy54224">
      <meta itemprop="description" content="个人学习笔记记录">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="wavy的流动小屋">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          从PCA到概率图模型做Multi-view Outlier Detection（1）
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-11-21 22:29:52" itemprop="dateCreated datePublished" datetime="2019-11-21T22:29:52+08:00">2019-11-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-11-23 12:32:55" itemprop="dateModified" datetime="2019-11-23T12:32:55+08:00">2019-11-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/maching-learning/" itemprop="url" rel="index">
                    <span itemprop="name">maching learning</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="principal-component-analysispca">Principal Component Analysis(PCA)</h2>
<h3 id="问题分析">问题分析</h3>
<p>主成分分析——寻找一个超平面，尽量将所有样本正确表达。 <a id="more"></a></p>
<blockquote>
<ol type="1">
<li><strong>最近重构性</strong>：样本点到这个超平面的目标足够近；</li>
<li><strong>最大可分性</strong>：略。</li>
</ol>
</blockquote>
<p>从<em>最近重构性</em>的角度来说，PCA的目标函数可以构造如下：</p>
<p><span class="math display">\[
\begin{aligned}
    \min\limits_{\mathbf{A}, \mathbf{E}}&amp;\quad\|\mathbf{E}\|_F\\
    \text{subject to}&amp;\quad \mathop{rank}(\mathbf{A})\leq r, \mathbf{D} = \mathbf{A} + \mathbf{E}
\end{aligned}
\]</span></p>
<h3 id="优化">优化</h3>
<p>假设投影的低维超平面由<span class="math inline">\(r\)</span>个正交的单位坐标<span class="math inline">\(\mathbf{W} = \{\mathbf{w}_1, \mathbf{w}_2, ..., \mathbf{w}_r\}\in\mathbb{R}^{d\times r}\)</span>构成，那么存在投影关系<span class="math inline">\(\mathbf{a}_i=\mathbf{W}\mathbf{z}_i\)</span>，<span class="math inline">\(\mathbf{z}_i\in\mathbb{R}^r\)</span>是<span class="math inline">\(\mathbf{d}_i\)</span>在超平面<span class="math inline">\(\mathbf{W}\)</span>上的投影。而<span class="math inline">\(\mathbf{z}_i\)</span>又可以通过<span class="math inline">\(\mathbf{d}_i\)</span>和<span class="math inline">\(\mathbf{W}\)</span>的列向量逐一点积得到<span class="math inline">\(\mathbf{z}_i = \mathbf{W}^T\mathbf{d}_i\)</span>，有<span class="math inline">\(\mathbf{A} = \mathbf{W}\mathbf{W}^T\mathbf{D}\)</span>。退而求其次，如果优化error项的平方，优化目标可以变为： <span class="math display">\[
\begin{aligned}
    \|\mathbf{E}\|_F^2 &amp;= \|\mathbf{D} - \mathbf{A}\|_F^2\\
    &amp;= \mathop{Tr}[(\mathbf{D} - \mathbf{A})^T(\mathbf{D} - \mathbf{A})]\\
    &amp;= \mathop{Tr}(\mathbf{D}^T\mathbf{D} - 2\mathbf{A}^T\mathbf{D} + \mathbf{A}^T\mathbf{A})\\
    &amp;= \mathop{Tr}(\mathbf{D}^T\mathbf{D} - 2\mathbf{D}^T\mathbf{W}\mathbf{W}^T\mathbf{D} + \mathbf{D}^T\mathbf{W}\mathbf{W}^T\mathbf{W}\mathbf{W}^T\mathbf{D})\\
    &amp;= \mathop{Tr}(\mathbf{D}^T\mathbf{D} - 2\mathbf{D}^T\mathbf{W}\mathbf{W}^T\mathbf{D} + \mathbf{D}^T\mathbf{W}\mathbf{\mathrm{I}}\mathbf{W}^T\mathbf{D})\\
    &amp;= \mathop{Tr}(\mathbf{D}^T\mathbf{D} - \mathbf{D}^T\mathbf{W}\mathbf{W}^T\mathbf{D})
\end{aligned}
\]</span> 因此，最终优化的目标函数为： <span class="math display">\[
\begin{aligned}
    \max\limits_{\mathbf{W}}&amp;\quad\mathop{Tr}(\mathbf{W}^T\mathbf{D}\mathbf{D}^T\mathbf{W})\\
    \text{subject to}&amp;\quad\mathbf{W}^T\mathbf{W} = \mathbf{\mathrm{I}}
\end{aligned}
\]</span> 这是一个很典型的谱分解求解的问题，套一下Ky Fan's Theorem直接SVD分解就好了。</p>
<h2 id="robust-principal-component-analysisrpca">Robust Principal Component Analysis(RPCA)</h2>
<h3 id="问题分析-1">问题分析</h3>
<p>加上子空间学习常用的策略——<strong>低秩</strong>+<strong>稀疏</strong>： <span class="math display">\[
\begin{aligned}
    \min\limits_{\mathbf{A}, \mathbf{E}}&amp;\quad\|\mathbf{A}\|_* + \lambda\|\mathbf{E}\|_1\\
    \text{subject to}&amp;\quad\mathbf{D} = \mathbf{A} + \mathbf{E}
\end{aligned}
\]</span></p>
<h3 id="优化-1">优化</h3>
<p>带等式约束的问题，直接用梯度上升法最大化对偶问题，增广拉格朗日函数为： <span class="math display">\[
\mathop{L}(\mathbf{A}, \mathbf{E}, \mathbf{Y}, \mathbf{\mu}) = \|\mathbf{A}\|_* + \lambda\|\mathbf{E}\|_1 + &lt;\mathbf{Y}, \mathbf{D} - \mathbf{A} - \mathbf{E}&gt; + \frac{\mu}{2}\|\mathbf{D} - \mathbf{A} - \mathbf{E}\|_F^2
\]</span> 定义soft-thresholding(shrinkage)函数： <span class="math display">\[
\mathop{\mathcal{S}_{\epsilon}}[x] = \begin{cases}
    x - \epsilon,&amp;\text{若}x &gt; \epsilon\\
    x + \epsilon,&amp;\text{若}x &lt; -\epsilon\\
    0,&amp;\text{其他}
\end{cases}
\]</span> 其中<span class="math inline">\(x\in\mathbb{R}\)</span>和<span class="math inline">\(\epsilon&gt;0\)</span>，可以将这个操作扩展到矩阵上，对矩阵逐元素操作。有下面的结论： <span class="math display">\[
\begin{aligned}
    \mathbf{U}\mathop{\mathcal{S}_{\epsilon}}[\mathbf{S}]\mathbf{V}^T &amp;= \mathop{\mathrm{argmin}}\limits_{\mathbf{X}}\;\epsilon\|\mathbf{X}\|_*+\frac{1}{2}\|\mathbf{X} - \mathbf{W}\|_F^2\\
    \mathop{\mathcal{S}_{\epsilon}}[\mathbf{W}] &amp;= \mathop{\mathrm{argmin}}\limits_{\mathbf{X}}\;\epsilon\|\mathbf{X}\|_1+\frac{1}{2}\|\mathbf{X} - \mathbf{W}\|_F^2
\end{aligned}
\]</span> 其中，<span class="math inline">\(\mathbf{U}\mathbf{S}\mathbf{V}^T\)</span>是<span class="math inline">\(\mathbf{W}\)</span>的SVD分解。</p>
<p>在ALM算法中，每次迭代先最小化原问题的所有变量，然后对参数<span class="math inline">\(\mathbf{Y}\)</span>更新步长为<span class="math inline">\(\mu\)</span>的梯度，最后更新步长<span class="math inline">\(\mu\)</span>（一般就是乘一个大于<span class="math inline">\(1\)</span>的数）。已有以下的结论[1]：</p>
<ul>
<li>更新过程中，当<span class="math inline">\(\mu\)</span>有界算法保证Q-linearly收敛到最优值，当<span class="math inline">\(\mu\)</span>无界保证super-Q-linearly收敛；</li>
<li>最优步长已被证明为<span class="math inline">\(\mu\)</span>；</li>
<li>即使<span class="math inline">\(\mu\)</span>在迭代中没有接近正无穷，算法依旧能保证收敛到最优。</li>
</ul>
<p>在Inexact的ALM算法中，不用每次外部迭代都将<span class="math inline">\(\mathbf{A}\)</span>和<span class="math inline">\(\mathbf{E}\)</span>用坐标上升直到收敛，只需要分别更新一次即可（原文证明了收敛性）。</p>
<h3 id="收敛条件">收敛条件</h3>
<p>算法收敛需要满足以下KKT条件： <span class="math display">\[
\begin{aligned}
    \mathbf{D} - \mathbf{A}^* - \mathbf{E}^* = 0\\
    \mathbf{Y}^* \in\partial\|\mathbf{A}^*\|_*\\
    \mathbf{Y}^* \in\partial\|\lambda\mathbf{E}^*\|_1\\
\end{aligned}
\]</span> 第二项和第三项是分别对<span class="math inline">\(\mathbf{A}\)</span>和<span class="math inline">\(\mathbf{E}\)</span>求导得到的（这里表示<span class="math inline">\(\mathbf{Y}\)</span>属于它们的次梯度）。实际代码来看论文中对第二和第三个条件放缩得到的式子好像有点不靠谱，用第一个式子就够了。</p>
<h2 id="probabilistic-principal-component-analysisppca">Probabilistic Principal Component Analysis(PPCA)</h2>
<p>这一块主要是从generation和inference的角度来解释PCA（参考PRML第12章）。</p>
<h3 id="生成过程">生成过程</h3>
<p>和前面类似，数据<span class="math inline">\(\mathbf{d}\)</span>是从latent space的向量<span class="math inline">\(\mathbf{z}\)</span>经过一个变换矩阵<span class="math inline">\(\mathbf{W}\)</span>的得到的。 <span class="math display">\[
\begin{aligned}
    \mathop{p}(\mathbf{z})\sim\mathcal{N}(0, \mathrm{I})\\
    \mathop{p}(\mathbf{d}|\mathbf{z}; \mathbf{W}, \mathbf{\mu}, \mathbf{\sigma})\sim\mathcal{N}(\mathbf{W}\mathbf{z} + \mathbf{\mu}, \mathbf{\sigma}^2\mathrm{I})
\end{aligned}
\]</span></p>
<h3 id="推断过程">推断过程</h3>
<h4 id="边际概率计算">边际概率计算</h4>
<p><span class="math display">\[
\mathop{p}(\mathbf{d};\mathbf{W}, \mathbf{\mu}, \mathbf{\sigma}) = \int\mathop{P}(\mathbf{d}|\mathbf{z}; \mathbf{W}, \mathbf{\mu}, \mathbf{\sigma})\mathop{P}(\mathbf{z})\,d\mathbf{z}
\]</span> 从<span class="math inline">\(\mathbf{d} = \mathbf{W}\mathbf{z} + \mathbf{\mu} + \mathbf{\epsilon}\)</span>的角度来说，这是关于<span class="math inline">\(\mathbf{z}\)</span>和<span class="math inline">\(\mathbf{\epsilon}\)</span>的一个线性变换，因此结果仍然是一个高斯分布。主要由下面推论得到：</p>
<blockquote>
<p>假设<span class="math inline">\(\mathbf{x}\sim\mathcal{N}(\mathbf{m}_{\mathbf{x}}, \mathbf{\Sigma}_{\mathbf{x}})\)</span>和<span class="math inline">\(\mathbf{y}\sim\mathcal{N}(\mathbf{m}_{\mathbf{y}}, \mathbf{\Sigma}_{\mathbf{y}})\)</span>，那么有（均值和方差直接套定义算期望就可以得到） <span class="math display">\[
\mathbf{Ax} + \mathbf{By} + \mathbf{c}\sim\mathcal{N}(\mathbf{Am}_{\mathbf{x}} + \mathbf{Bm}_{\mathbf{y}} + \mathbf{c}, \mathbf{A}\mathbf{\Sigma}_{\mathbf{x}}\mathbf{A}^T + \mathbf{B}\mathbf{\Sigma}_{\mathbf{y}}\mathbf{B}^T)
\]</span></p>
</blockquote>
<p>因此边际概率为 <span class="math display">\[
\mathop{p}(\mathbf{d};\mathbf{W}, \mathbf{\mu}, \mathbf{\sigma})\sim\mathcal{N}(\mathbf{\mu}, \mathbf{W}\mathbf{W}^T + \mathbf{\sigma}^2\mathrm{I})
\]</span></p>
<h4 id="后验概率计算">后验概率计算</h4>
<p>后验概率可以直接硬解： <span class="math display">\[
\begin{aligned}
\mathop{p}(\mathbf{z}|\mathbf{d}) = \frac{\mathop{P}(\mathbf{d}|\mathbf{z}; \mathbf{W}, \mathbf{\mu}, \mathbf{\sigma})P(\mathbf{z})}{P_{\mathbf{W}, \mathbf{\mu}, \mathbf{\sigma}}(\mathbf{d})}
\end{aligned}
\]</span> 高斯分布的乘法也是一个高斯分布（除法类推），只关注上面式子的指数项： <span class="math display">\[
\begin{aligned}
&amp;\frac{1}{\mathbf{\sigma}^2}(\mathbf{d} - \mathbf{W}\mathbf{z} - \mathbf{\mu})^T(\mathbf{d} - \mathbf{W}\mathbf{z} - \mathbf{\mu}) + \mathbf{z}^T\mathbf{z} - (\mathbf{d} - \mathbf{\mu})^T(\mathbf{W}\mathbf{W}^T + \mathbf{\sigma}^2\mathrm{I})^{-1}(\mathbf{d} - \mathbf{\mu})\\
\sim&amp;\frac{1}{\mathbf{\sigma}^2}\{[\mathbf{W}\mathbf{z} - (\mathbf{d} - \mathbf{\mu})]^T[\mathbf{W}\mathbf{z} - (\mathbf{d} - \mathbf{\mu})] + \mathbf{\sigma}^2\mathbf{z}^T\mathbf{z}\}\\
\sim&amp;\frac{1}{\mathbf{\sigma}^2}[\mathbf{z}^T(\mathbf{W}^T\mathbf{W} + \mathbf{\sigma}^2\mathrm{I})\mathbf{z} - (\mathbf{d} - \mathbf{\mu})^T\mathbf{W}\mathbf{z} - \mathbf{z}^T\mathbf{W}^T(\mathbf{d} - \mathbf{\mu})]
\end{aligned}
\]</span> 变量代换<span class="math inline">\(\mathbf{M} = \mathbf{W}^T\mathbf{W} + \mathbf{\sigma}^2\mathrm{I}\)</span>： <span class="math display">\[
\begin{aligned}
&amp;\text{上式}\\
\sim&amp;\frac{1}{\mathbf{\sigma}^2}[\mathbf{z}^T\mathbf{M}\mathbf{z} - (\mathbf{d} - \mathbf{\mu})^T\mathbf{W}\mathbf{z} - \mathbf{z}^T\mathbf{W}^T(\mathbf{d} - \mathbf{\mu})]\\
\sim&amp;\frac{1}{\mathbf{\sigma}^2}[\mathbf{z}^T\mathbf{M}\mathbf{z} - (\mathbf{d} - \mathbf{\mu})^T\mathbf{W}\mathbf{M}^{-1}\mathbf{M}\mathbf{z} - \mathbf{z}^T\mathbf{M}\mathbf{M}^{-1}\mathbf{W}^T(\mathbf{d} - \mathbf{\mu})]\\
\sim&amp;\frac{1}{\mathbf{\sigma}^2}[\mathbf{z} - \mathbf{M}^{-1}\mathbf{W}^T(\mathbf{d} - \mathbf{\mu})]^T\mathbf{M}[\mathbf{z} - \mathbf{M}^{-1}\mathbf{W}^T(\mathbf{d} - \mathbf{\mu})]
\end{aligned}
\]</span> 观察均值和方差，得到 <span class="math display">\[
\mathop{p}(\mathbf{z}|\mathbf{d})\sim\mathcal{N}(\mathbf{M}^{-1}\mathbf{W}^T(\mathbf{d} - \mathbf{\mu}), \mathbf{\sigma}^2\mathbf{M}^{-1})
\]</span></p>
<h3 id="优化-2">优化</h3>
<h4 id="最大似然">最大似然</h4>
<p>因为关于观测变量<span class="math inline">\(\mathbf{d}\)</span>的边际概率已经能准确推断出来了，因此可以直接对边际概率进行最大似然求解： <span class="math display">\[
\begin{aligned}
&amp;\log\mathop{p}(\mathbf{D}; \mathbf{W}, \mathbf{\mu}, \mathbf{\sigma})\\
=&amp; \sum_{i=1}^{\mathrm{n}}\log\mathop{p}(\mathbf{d}_i; \mathbf{W}, \mathbf{\mu}, \mathbf{\sigma})\\
=&amp; \sum_{i=1}^{\mathrm{n}}\log\{\frac{1}{(2\pi)^{\mathrm{d}/2}}\frac{1}{|\mathbf{W}\mathbf{W}^T + \mathbf{\sigma}^2\mathrm{I}|^{1/2}}\exp[-\frac{1}{2}(\mathbf{d}_i - \mathbf{\mu})^T{(\mathbf{W}\mathbf{W}^T + \mathbf{\sigma}^2\mathrm{I})}^{-1}(\mathbf{d}_i - \mathbf{\mu})]\}\\
=&amp; -\frac{\mathrm{nd}}{2}\log(2\pi) - \frac{\mathrm{n}}{2}\log|\mathbf{W}\mathbf{W}^T + \mathbf{\sigma}^2\mathrm{I}| - \frac{1}{2}\sum_{i=1}^{\mathrm{n}}(\mathbf{d}_i - \mathbf{\mu})^T{(\mathbf{W}\mathbf{W}^T + \mathbf{\sigma}^2\mathrm{I})}^{-1}(\mathbf{d}_i - \mathbf{\mu})
\end{aligned}
\]</span> 对<span class="math inline">\(\mathbf{\mu}\)</span>求导，令导数为<span class="math inline">\(0\)</span>，得： <span class="math display">\[
-{(\mathbf{W}\mathbf{W}^T + \mathbf{\sigma}^2\mathrm{I})}^{-1}\sum_{i=1}^{\mathrm{n}}(\mathbf{\mu} - \mathbf{d}_i) = 0
\]</span> 即<span class="math inline">\(\mathbf{\mu}\)</span>是<span class="math inline">\(\mathbf{d}_i\)</span>的算术均值。那么 <span class="math display">\[
\begin{aligned}
&amp;\text{对数似然目标函数}\\
=&amp;-\frac{\mathrm{nd}}{2}\log(2\pi) - \frac{\mathrm{n}}{2}\log|\mathbf{W}\mathbf{W}^T + \mathbf{\sigma}^2\mathrm{I}| - \frac{1}{2}\sum_{i=1}^{\mathrm{n}}\mathop{tr}[(\mathbf{d}_i - \mathbf{\mu})^T{(\mathbf{W}\mathbf{W}^T + \mathbf{\sigma}^2\mathrm{I})}^{-1}(\mathbf{d}_i - \mathbf{\mu})]\\
=&amp;-\frac{\mathrm{nd}}{2}\log(2\pi) - \frac{\mathrm{n}}{2}\log|\mathbf{W}\mathbf{W}^T + \mathbf{\sigma}^2\mathrm{I}| - \frac{1}{2}\sum_{i=1}^{\mathrm{n}}\mathop{tr}[{(\mathbf{W}\mathbf{W}^T + \mathbf{\sigma}^2\mathrm{I})}^{-1}(\mathbf{d}_i - \mathbf{\mu})(\mathbf{d}_i - \mathbf{\mu})^T]\\
=&amp;-\frac{\mathrm{nd}}{2}\log(2\pi) - \frac{\mathrm{n}}{2}\log|\mathbf{W}\mathbf{W}^T + \mathbf{\sigma}^2\mathrm{I}| - \frac{\mathrm{n}}{2}\mathop{tr}\{ {(\mathbf{W}\mathbf{W}^T + \mathbf{\sigma}^2\mathrm{I})}\frac{1}{\mathrm{n}}\sum_{i=1}^{\mathrm{n}}[(\mathbf{d}_i - \mathbf{\mu})(\mathbf{d}_i - \mathbf{\mu})^T]\}\\
=&amp;-\frac{\mathrm{n}}{2}\{\mathrm{d}\log(2\pi) + \log|\mathbf{W}\mathbf{W}^T + \mathbf{\sigma}^2\mathrm{I}| + \mathop{tr}[{(\mathbf{W}\mathbf{W}^T + \mathbf{\sigma}^2\mathrm{I})}^{-1}\mathbf{\Sigma}]\}
\end{aligned}
\]</span> 其中，<span class="math inline">\(\mathbf{\Sigma}\)</span>是<span class="math inline">\(\mathbf{d}_i\)</span>的协方差矩阵，定义为<span class="math inline">\(\mathbf{\Sigma} = \frac{1}{\mathrm{n}}\sum_{i=1}^{\mathrm{n}}[(\mathbf{d}_i - \mathbf{\mu})(\mathbf{d}_i - \mathbf{\mu})^T]\)</span>。而<span class="math inline">\(\mathbf{W}\)</span>在上面函数的驻点被证明是： <span class="math display">\[
\begin{aligned}
    \mathbf{W}_{ML} = \mathbf{U}_{M}(\mathbf{L}_M-\mathbf{\sigma}^2\mathrm{I})^{1/2}\mathbf{R}
\end{aligned}
\]</span> <span class="math inline">\(\mathbf{U}_{M}\)</span>和<span class="math inline">\(\mathop{diag}(\mathbf{L}_M)\)</span>是<span class="math inline">\(\mathbf{\Sigma}\)</span>的任意特征向量以及对应特征值子集。如果是将维度从<span class="math inline">\(\mathrm{d}\)</span>降到<span class="math inline">\(\mathrm{r}\)</span>，那么<span class="math inline">\(\mathbf{U}_{M}\in\mathbb{R}^{\mathrm{d}\times \mathrm{c}}\)</span>。<span class="math inline">\(\mathrm{R}\)</span>是任意正交矩阵（看成在latent space的旋转变换）。对应的<span class="math inline">\(\mathbf{\sigma}\)</span>最大似然解为： <span class="math display">\[
\mathbf{\sigma}^2_{ML} = \frac{1}{\mathrm{d} - \mathrm{r}}\sum_{i=\mathrm{r} + 1}^{\mathrm{d}}\mathbf{\lambda}_i
\]</span> 即<span class="math inline">\(\mathbf{\sigma}^2\)</span>为剩余<span class="math inline">\(\mathrm{d} - \mathrm{r}\)</span>个特征值的均值。按照个人看论文和PRML的理解，这个解的含义是：</p>
<ul>
<li>将原始数据用一个高斯函数拟合，学习出来的协方差<span class="math inline">\((\mathbf{W}\mathbf{W}^T + \mathbf{\sigma}^2\mathrm{I})^{-1}\)</span>可以看成是原协方差<span class="math inline">\(\mathbf{\Sigma}\)</span>的一种低秩近似的形式。保留协方差<span class="math inline">\(\mathrm{r}\)</span>个特征部分，剩下的特征值使用一个大小为<span class="math inline">\(\mathbf{\sigma}^2\)</span>的各向同性协方差来近似（换言之，当<span class="math inline">\(\mathrm{r} = \mathrm{d}\)</span>时，直接使用原协方差矩阵）。</li>
<li>从自由度的角度来说，原协方差矩阵提供了<span class="math inline">\(\frac{\mathrm{d}(\mathrm{d} - 1)}{2}\)</span>个可调的变量（协方差矩阵的对称性），各向同性的协方差矩阵提供了<span class="math inline">\(\mathrm{d}\)</span>个可调的变量，而PPCA提供的自由度为<span class="math inline">\(\mathrm{d}\mathrm{r} + 1 - \frac{\mathrm{r}(\mathrm{r} - 1)}{2}\)</span>减去的一项是由于latent space的旋转不变性（<span class="math inline">\(\mathbf{R}\)</span>矩阵导致的）。</li>
</ul>
<p>结合前面的后验概率推算的公式，我们就可以直接推出变量在latent space的分布了。</p>
<h4 id="em算法">EM算法</h4>
<p>带隐变量的最大似然问题一般都可以往EM算法的角度来考虑（只要后验概率能显式推出来，不然就要用variational inference顺便估计一下后验概率——当然也可以采样）。现在最大似然已经能显示求解，那么用EM算法去逼近最大似然还有什么优势吗？就PPCA来说，可能是一些复杂度上的优势。在一些别的问题上，直接最大似然可能没有解析解（这是很常见的），那么梯度下降或者是坐标上升法去逼近就是很正常的方法了。先拆出ELBO项和KL散度项： <span class="math display">\[
\small
\begin{aligned}
&amp;\log\mathop{p}(\mathbf{D};\mathbf{W}, \mathbf{\mu}, \mathbf{\sigma})\\
=&amp; \int\mathop{q}(\mathbf{Z}; \mathbf{D})\log\frac{\mathop{p}(\mathbf{D}, \mathbf{Z};\mathbf{W}, \mathbf{\mu}, \mathbf{\sigma})}{\mathop{q}(\mathbf{Z}; \mathbf{D})}\,d\mathbf{Z} + \mathop{KL}[\mathop{q}(\mathbf{Z}; \mathbf{D})\|\mathop{p}(\mathbf{Z}|\mathbf{D}; \mathbf{W}, \mathbf{\sigma})]\\
=&amp; \int\mathop{q}(\mathbf{Z}; \mathbf{D})[\sum_{i=1}^{\mathrm{n}}\log\mathop{p}(\mathbf{d}_i, \mathbf{z}_i;\mathbf{W}, \mathbf{\mu}, \mathbf{\sigma}) - \log\mathop{q}(\mathbf{Z}; \mathbf{D})]\,d\mathbf{Z} + \mathop{KL}[\mathop{q}(\mathbf{Z}; \mathbf{D})\|\prod_{i=1}^{\mathrm{n}}\mathop{p}(\mathbf{z}_i|\mathbf{d}_i; \mathbf{W}, \mathbf{\sigma})]
\end{aligned}
\]</span> 其中， <span class="math display">\[
\begin{aligned}
&amp;\log\mathop{p}(\mathbf{d}_i, \mathbf{z}_i;\mathbf{W}, \mathbf{\mu}, \mathbf{\sigma})\\
=&amp;\log\mathop{p}(\mathbf{d}_i| \mathbf{z}_i;\mathbf{W}, \mathbf{\mu}, \mathbf{\sigma}) + \log\mathop{p}(\mathbf{z}_i)\\
=&amp;-\frac{\mathrm{d}}{2}\log(2\pi\mathbf{\sigma}^2) - \frac{1}{2\mathbf{\sigma}^2}\|\mathbf{d} - \mathbf{W}\mathbf{z}_i - \mathbf{\mu}\|_2^2 -\frac{\mathrm{r}}{2}\log(2\pi) -  \frac{1}{2}\mathbf{z}_i^T\mathbf{z}_i
\end{aligned}
\]</span></p>
<ul>
<li><p>E步，将<span class="math inline">\(\mathop{q}(\mathbf{Z}; \mathbf{D})\)</span>用当前的<span class="math inline">\(\prod_{i=1}^{\mathrm{n}}\mathop{p}(\mathbf{z}_i|\mathbf{d}_i; \mathbf{W}_{old}, \mathbf{\sigma}_{old})\)</span>代替。具体而言，我们前面已经计算出对于每个点的后验概率为 <span class="math display">\[
\mathop{p}(\mathbf{z}|\mathbf{d})\sim\mathcal{N}(\mathbf{M}^{-1}\mathbf{W}^T(\mathbf{d} - \mathbf{\mu}), \mathbf{\sigma}^2\mathbf{M}^{-1})
\]</span> 直接带入即可。</p></li>
<li><p>M步，最大化ELBO项： <span class="math display">\[
\scriptsize
\begin{aligned}
&amp;\int\mathop{q}(\mathbf{Z}; \mathbf{D})[\sum_{i=1}^{\mathrm{n}}\log\mathop{p}(\mathbf{d}_i, \mathbf{z}_i;\mathbf{W}, \mathbf{\mu}, \mathbf{\sigma}) - \log\mathop{q}(\mathbf{Z}; \mathbf{D})]\,d\mathbf{Z}\\
\sim&amp;\mathbb{E}_{\mathop{q}(\mathbf{Z}; \mathbf{D})}-\sum_{i=1}^{\mathrm{n}}[\frac{\mathrm{d}}{2}\log(2\pi\mathbf{\sigma}^2) + \frac{1}{2\mathbf{\sigma}^2}\|\mathbf{d}_i - \mathbf{W}\mathbf{z}_i - \mathbf{\mu}\|_2^2 +\frac{\mathrm{r}}{2}\log(2\pi) + \frac{1}{2}\mathbf{z}_i^T\mathbf{z}_i]\\
=&amp;\mathbb{E}_{\mathop{q}(\mathbf{Z}; \mathbf{D})}-\sum_{i=1}^{\mathrm{n}}[\frac{\mathrm{d}}{2}\log(2\pi\mathbf{\sigma}^2) + \frac{1}{2\mathbf{\sigma}^2}\|\mathbf{d}_i - \mathbf{\mu}\|_2^2 - \frac{1}{\mathbf{\sigma}^2}\mathbf{z}_i^T\mathbf{W}^T(\mathbf{d}_i - \mathbf{\mu}) + \frac{1}{2\mathbf{\sigma}^2}\mathop{tr}(\mathbf{W}^T\mathbf{W}\mathbf{z}_i\mathbf{z}_i^T) +\frac{\mathrm{r}}{2}\log(2\pi) + \frac{1}{2}\mathop{tr}(\mathbf{z}_i\mathbf{z}_i^T)]\\
=&amp;\!-\!\sum_{i=1}^{\mathrm{n}}\{\frac{\mathrm{d}}{2}\!\log( \mathbf{\sigma}^2) \!+\! \frac{1}{2\mathbf{\sigma}^2}\!\|\mathbf{d}_i - \mathbf{\mu}\|_2^2 \!-\! \frac{1}{\mathbf{\sigma}^2}\!\mathbb{E}_{\mathop{q}(\mathbf{Z}; \mathbf{D})}[\mathbf{z}_i]^T\mathbf{W}^T(\mathbf{d}_i - \mathbf{\mu}) \!+\! \frac{1}{2\mathbf{\sigma}^2}\!\!\mathop{tr}(\mathbf{W}^T\mathbf{W}\mathbb{E}_{\mathop{q}(\mathbf{Z}; \mathbf{D})}[\mathbf{z}_i\mathbf{z}_i^T]) \!+\! \frac{1}{2}\mathop{tr}(\mathbb{E}_{\mathop{q}(\mathbf{Z}; \mathbf{D})}[\mathbf{z}_i\mathbf{z}_i^T])\}
\end{aligned}
\]</span> 对应的均值和方差期望可以直接求解出来： <span class="math display">\[
\mathbb{E}_{\mathop{q}(\mathbf{Z}; \mathbf{D})}[\mathbf{z}_i] = \mathbf{M}_{old}^{-1}\mathbf{W}_{old}^T(\mathbf{d}_i - \overline{\mathbf{d}})
\]</span> <span class="math display">\[
\begin{aligned}
\mathbb{E}_{\mathop{q}(\mathbf{Z}; \mathbf{D})}[\mathbf{z}_i\mathbf{z}_i^T] &amp;= \mathop{cov}[\mathbf{z}_i] + \mathbb{E}_{\mathop{q}(\mathbf{Z}; \mathbf{D})}[\mathbf{z}_i]\mathbb{E}_{\mathop{q}(\mathbf{Z}; \mathbf{D})}[\mathbf{z}_i]^T\\
&amp;= \mathbf{\sigma}_{old}^2\mathbf{M}_{old}^{-1} + \mathbf{M}_{old}^{-1}\mathbf{W}_{old}^T(\mathbf{d}_i - \overline{\mathbf{d}})(\mathbf{d}_i - \overline{\mathbf{d}})^T\mathbf{W}_{old}\mathbf{M}_{old}^{-1}
\end{aligned}
\]</span> 和上面一样，<span class="math inline">\(\mathbf{M}_{old} = \mathbf{W}_{old}^T\mathbf{W}_{old} + \mathbf{\sigma}_{old}^2\mathrm{I}\)</span>。现在对<span class="math inline">\(\mathbf{W}\)</span>和<span class="math inline">\(\mathbf{\sigma}\)</span>最大化：</p>
<ul>
<li>对<span class="math inline">\(\mathbf{\sigma}^2\)</span>求导，令导数为<span class="math inline">\(0\)</span>： <span class="math display">\[
\small
-\!\sum_{i=1}^{\mathrm{n}}\{\frac{\mathrm{d}}{2\mathbf{\sigma}^2} \!-\! \frac{\|\mathbf{d}_i - \overline{\mathbf{d}}\|_2^2}{2\mathbf{\sigma}^4} \!+\! \frac{\mathbb{E}_{\mathop{q}(\mathbf{Z}; \mathbf{D})}[\mathbf{z}_i]^T\mathbf{W}^T(\mathbf{d}_i - \overline{\mathbf{d}})}{\mathbf{\sigma}^4} \!-\! \frac{\mathop{tr}(\mathbf{W}^T\mathbf{W}\mathbb{E}_{\mathop{q}(\mathbf{Z}; \mathbf{D})}[\mathbf{z}_i\mathbf{z}_i^T])}{2\mathbf{\sigma}^4}\} = 0
\]</span> 解得： <span class="math display">\[
\mathbf{\sigma}_{new}^2 = \frac{1}{\mathrm{nd}}\sum_{i=1}^{\mathrm{n}}\{\|\mathbf{d}_i - \overline{\mathbf{d}}\|_2^2 \!-\! 2\mathbb{E}_{\mathop{q}(\mathbf{Z}; \mathbf{D})}[\mathbf{z}_i]^T\mathbf{W}^T(\mathbf{d}_i - \overline{\mathbf{d}}) \!+\! \mathop{tr}(\mathbf{W}^T\mathbf{W}\mathbb{E}_{\mathop{q}(\mathbf{Z}; \mathbf{D})}[\mathbf{z}_i\mathbf{z}_i^T])\}
\]</span></li>
<li>对<span class="math inline">\(\mathbf{W}\)</span>求导，令导数为<span class="math inline">\(0\)</span>： <span class="math display">\[
\!-\!\sum_{i=1}^{\mathrm{n}}\{\frac{1}{\mathbf{\sigma}^2}\!(\mathbf{d}_i - \mathbf{\mu})\mathbb{E}_{\mathop{q}(\mathbf{Z}; \mathbf{D})}[\mathbf{z}_i]^T \!-\! \frac{1}{\mathbf{\sigma}^2}\!\!\mathbf{W}\mathbb{E}_{\mathop{q}(\mathbf{Z}; \mathbf{D})}[\mathbf{z}_i\mathbf{z}_i^T]\} = 0
\]</span> 解得： <span class="math display">\[
\small
\mathbf{W}_{new} = \{\sum_{i=1}^{\mathrm{n}}(\mathbf{d}_i - \mathbf{\mu})\mathbb{E}_{\mathop{q}(\mathbf{Z}; \mathbf{D})}[\mathbf{z}_i]^T\}\{\sum_{i=1}^{\mathrm{n}}\mathbb{E}_{\mathop{q}(\mathbf{Z}; \mathbf{D})}[\mathbf{z}_i\mathbf{z}_i^T]\}^{-1}
\]</span> 更新<span class="math inline">\(\mathbf{W}\)</span>和更新<span class="math inline">\(\mathbf{\sigma}\)</span>无关，因此先更新<span class="math inline">\(\mathbf{W}\)</span>，再更新<span class="math inline">\(\mathbf{\sigma}\)</span>。</li>
</ul></li>
</ul>
<h2 id="引用文献">引用文献</h2>
<p>[1] Lin Z, Chen M, Ma Y. The augmented lagrange multiplier method for exact recovery of corrupted low-rank matrices[J]. arXiv preprint arXiv:1009.5055, 2010.</p>
<p>[2] Tipping M E, Bishop C M. Probabilistic principal component analysis[J]. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 1999, 61(3): 611-622.</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/PCA/" rel="tag"># PCA</a>
              <a href="/tags/outlier-detection/" rel="tag"># outlier detection</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
                <a href="/2018/05/24/KMeans2Ncut/" rel="next" title="从KMeans到Ncut">
                  <i class="fa fa-chevron-left"></i> 从KMeans到Ncut
                </a>
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
            </div>
          </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#principal-component-analysispca"><span class="nav-number">1.</span> <span class="nav-text">Principal Component Analysis(PCA)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#问题分析"><span class="nav-number">1.1.</span> <span class="nav-text">问题分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#优化"><span class="nav-number">1.2.</span> <span class="nav-text">优化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#robust-principal-component-analysisrpca"><span class="nav-number">2.</span> <span class="nav-text">Robust Principal Component Analysis(RPCA)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#问题分析-1"><span class="nav-number">2.1.</span> <span class="nav-text">问题分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#优化-1"><span class="nav-number">2.2.</span> <span class="nav-text">优化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#收敛条件"><span class="nav-number">2.3.</span> <span class="nav-text">收敛条件</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#probabilistic-principal-component-analysisppca"><span class="nav-number">3.</span> <span class="nav-text">Probabilistic Principal Component Analysis(PPCA)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#生成过程"><span class="nav-number">3.1.</span> <span class="nav-text">生成过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#推断过程"><span class="nav-number">3.2.</span> <span class="nav-text">推断过程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#边际概率计算"><span class="nav-number">3.2.1.</span> <span class="nav-text">边际概率计算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#后验概率计算"><span class="nav-number">3.2.2.</span> <span class="nav-text">后验概率计算</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#优化-2"><span class="nav-number">3.3.</span> <span class="nav-text">优化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#最大似然"><span class="nav-number">3.3.1.</span> <span class="nav-text">最大似然</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#em算法"><span class="nav-number">3.3.2.</span> <span class="nav-text">EM算法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#引用文献"><span class="nav-number">4.</span> <span class="nav-text">引用文献</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <img class="site-author-image" itemprop="image" alt="wy54224"
    src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">wy54224</p>
  <div class="site-description" itemprop="description">个人学习笔记记录</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/wy54224" title="GitHub &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;wy54224" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/mailto:wy54224@gmail.com" title="E-Mail &amp;rarr; mailto:wy54224@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">wy54224</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.5.0
  </div>

        












        
      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/pjax/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>
  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.getAttribute('pjax') !== null) {
      element.setAttribute('pjax', '');
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  















    <div id="pjax">

  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  

    </div>
</body>
</html>
